{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Anime Planet // Vulpix // CS109 Final Project\n",
    "Tuongvan Le, Tiffany Lee, Kelwen Peng, Dylan Tan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Scraping Data\n",
    "###Reviews and Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# The \"requests\" library makes working with HTTP requests easier than the built-in urllib libraries.\n",
    "import requests\n",
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = open(\"../data/anime_pages.p\",'rb') \n",
    "anime = pickle.load(test1)  \n",
    "test1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = 'http://www.anime-planet.com'\n",
    "\n",
    "all_anime = []\n",
    "for page in anime:\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    animelist = soup.find('ul',attrs={\"class\": \"cardDeck pure-g cd-narrow\"}).find_all(\"li\")\n",
    "    for ani in animelist:\n",
    "        all_anime.append(root + ani.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "CPU times: user 5min 47s, sys: 4min 43s, total: 10min 30s\n",
      "Wall time: 4h 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anime_reviews = defaultdict(list)\n",
    "stop = 0\n",
    "for i in xrange(len(all_anime)):\n",
    "    if i>=stop:\n",
    "        print i\n",
    "        stop+=500\n",
    "    review = requests.get(all_anime[i]+\"/reviews\")\n",
    "    soup1 = BeautifulSoup(review.text, \"html.parser\")\n",
    "    pages = soup1.find('section',attrs={'pure-g'})\n",
    "    if pages:\n",
    "        pages = pages.find('div',attrs='pure-1 md-4-5')\n",
    "    if pages:\n",
    "        pages = pages.find('div',attrs='pagination aligncenter')\n",
    "        if pages:\n",
    "            all_li = pages.ul.find_all('li')\n",
    "            maxpage = int(all_li[len(all_li)-2].a.contents[0])\n",
    "        else:\n",
    "            maxpage=1\n",
    "    else:\n",
    "        maxpage=1\n",
    "    for j in xrange(1,maxpage+1):\n",
    "        anime_reviews[all_anime[i].split('/')[-1]].append(requests.get(all_anime[i]+\"/reviews?page=\"+str(j)))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle.dump(anime_reviews, open(\"../data/anime_reviews1.p\", \"wb\"))\n",
    "\n",
    "test3 = open(\"../data/anime_reviews1.p\",'rb') \n",
    "anime_reviews = pickle.load(test3)  \n",
    "test3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewsdict = {}\n",
    "for anime in anime_reviews.keys():\n",
    "    animedict = {anime:{}}\n",
    "    for a in range(len(anime_reviews[anime])):\n",
    "        soup = BeautifulSoup(anime_reviews[anime][a].text, \"html.parser\")\n",
    "        body = soup.find('div',attrs={\"class\": \"pure-1 md-4-5\"})\n",
    "        if body is not None:\n",
    "            sections = body.find_all('section', attrs={'class':'pure-g'})\n",
    "            names = [i.find('a',attrs={'itemprop':'author'}).contents[0] for i in sections if i.find('a',attrs={'itemprop':'author'}) is not None]\n",
    "            scores = [i.find_all('div',attrs={'class':'pure-1-5'}) for i in sections if i.find_all('div',attrs={'class':'pure-1-5'}) != []]\n",
    "            story = [i[0].contents[0] for i in scores]\n",
    "            animation = [i[1].contents[0] for i in scores]\n",
    "            sound = [i[2].contents[0] for i in scores]\n",
    "            characters = [i[3].contents[0] for i in scores]\n",
    "            overall = [i[4].contents[0] for i in scores]\n",
    "            reviews = [i.contents[1].string for i in body.find_all('section', attrs={'class':'userContent readMore'})]\n",
    "            anime_name = soup.find('h1',attrs={\"itemprop\": \"name\"}).contents*len(names)\n",
    "            temp = dict(zip(names,zip(story,animation,sound,characters,overall,reviews,anime_name)))\n",
    "            animedict[anime].update(temp)\n",
    "    reviewsdict.update(animedict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/all_reviews.json', 'w') as fp:\n",
    "      json.dump(reviewsdict, fp)\n",
    "\n",
    "with open('../data/all_reviews.json','r') as data_file:    \n",
    "    reviewsdict = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewlist = []\n",
    "for i in reviewsdict.keys():\n",
    "    reviewlist+=reviewsdict[i].keys()\n",
    "users = list(set(reviewlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4462, 17194)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users),len(reviewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newlist = []\n",
    "\n",
    "for key in reviewsdict:\n",
    "    for user in reviewsdict[key]:\n",
    "        story = reviewsdict[key][user][0]\n",
    "        animation = reviewsdict[key][user][1]\n",
    "        sound = reviewsdict[key][user][2]\n",
    "        character = reviewsdict[key][user][3]\n",
    "        overall = reviewsdict[key][user][4]\n",
    "        reviews = reviewsdict[key][user][5]\n",
    "        anime_name = reviewsdict[key][user][6]\n",
    "        temp = {\"name\":user,'anime':key,'story':story,'animation':animation,'sound':sound,'character':character,'overall':overall,'reviews':reviews,'anime_name':anime_name}\n",
    "        newlist.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17194, 9)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = pd.DataFrame(newlist)\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata.to_csv('../data/all_reviews.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#newdata.loc[:,['anime','anime_name']].to_csv('../data/anime_name.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 26.8 s, total: 3min 44s\n",
      "Wall time: 1h 46min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = 'http://www.anime-planet.com/users/'\n",
    "user_profiles = {}\n",
    "for i in xrange(len(users)):\n",
    "    user = requests.get(root+users[i])\n",
    "    soup = BeautifulSoup(user.text, \"html.parser\")\n",
    "    user_profiles[users[i]]=soup\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "each_user_info = {}\n",
    "\n",
    "for user in user_profiles.keys():\n",
    "    test = user_profiles[user]\n",
    "    user_info = {}\n",
    "\n",
    "    info = test.find('div',attrs={'class':'pure-u-3-5 pure-u-md-4-5'})\n",
    "    name = info.find('h2',attrs={'id':'profileName'}).find_all('a')[1].contents[0]\n",
    "\n",
    "    stats = info.find('ul',attrs={'class':'userStats'}).findAll('li')\n",
    "    place = \"\"\n",
    "    age_sex = \"\"\n",
    "    join = \"\"\n",
    "\n",
    "    # find place, age, sex if exist\n",
    "    for li in stats:\n",
    "        if li.findAll(attrs={\"class\" : \"fa fa-home\"}):\n",
    "            place = li.contents[1]\n",
    "        elif li.findAll(attrs={\"class\" : \"fa fa-calendar\"}):\n",
    "            join = li.contents[1]\n",
    "        elif li.findAll(attrs={\"class\" : \"fa fa-user\"}):\n",
    "            age_sex = li.contents[2].replace('\\n','')\n",
    "\n",
    "    each_user_info[name] = {\"place\":place,\"age_sex\":age_sex,\"join\":join}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('../data/all_users.json', 'w') as fp:\n",
    "#        json.dump(each_user_info, fp)\n",
    "\n",
    "with open('../data/all_users.json','r') as data_file:    \n",
    "    each_user_info = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_sex</th>\n",
       "      <th>join</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00081</th>\n",
       "      <td>26</td>\n",
       "      <td>Joined Jan 16, 2012</td>\n",
       "      <td>Death Star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0Marcandre</th>\n",
       "      <td>20 / M</td>\n",
       "      <td>Joined Mar 31, 2013</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0oDaano0</th>\n",
       "      <td>?</td>\n",
       "      <td>Joined Apr 9, 2013</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0utl4w</th>\n",
       "      <td>?</td>\n",
       "      <td>Joined Mar 31, 2010</td>\n",
       "      <td>somewhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11bowman</th>\n",
       "      <td>21 / M</td>\n",
       "      <td>Joined Jun 19, 2010</td>\n",
       "      <td>Amsterdam, Netherlands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age_sex                 join                   place\n",
       "00081           26   Joined Jan 16, 2012              Death Star\n",
       "0Marcandre  20 / M   Joined Mar 31, 2013                        \n",
       "0oDaano0         ?    Joined Apr 9, 2013                        \n",
       "0utl4w           ?   Joined Mar 31, 2010               somewhere\n",
       "11bowman    21 / M   Joined Jun 19, 2010  Amsterdam, Netherlands"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf = pd.DataFrame(each_user_info).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userdf.to_csv('../data/all_users.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Anime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "\n",
    "# def install(package):\n",
    "#    pip.main(['install', package])\n",
    "\n",
    "# install('python-mal')\n",
    "\n",
    "# import myanimelist.session\n",
    "# #username=\"teamvulpix\", password=\"2015harvardcs109\"\n",
    "# session = myanimelist.session.Session()\n",
    "# session.login()\n",
    "# bebop = session.anime(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping the search bar of www.anime-planet.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#48 animes per page with 152 pages = ~7296\n",
    "anime = []\n",
    "for i in xrange(1,153):\n",
    "    anime.append(requests.get('http://www.anime-planet.com/anime/all?sort=title&order=asc&page='+str(i)))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding all anime links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'http://www.anime-planet.com'\n",
    "\n",
    "all_anime = []\n",
    "for page in anime:\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    animelist = soup.find('ul',attrs={\"class\": \"cardDeck pure-g cd-narrow\"}).find_all(\"li\")\n",
    "    for ani in animelist:\n",
    "        all_anime.append(root + ani.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting each anime page information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "each_anime = []\n",
    "\n",
    "for i in xrange(len(all_anime)):\n",
    "    each_anime.append(requests.get(all_anime[i]))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Getting info from each anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "each_anime_info = {}\n",
    "\n",
    "for anim in each_anime:\n",
    "    anime_info = {}\n",
    "    soup1 = BeautifulSoup(anim.text, \"html.parser\")\n",
    "    name = soup1.find('h1',attrs={\"itemprop\": \"name\"}).contents[0]\n",
    "    altname = soup1.find('h2',attrs={\"class\": \"aka\"})\n",
    "\n",
    "    infos = soup1.find_all('div',attrs={\"class\": \"pure-u-1 pure-u-md-1-5\"})\n",
    "    atype = infos[0]\n",
    "    studio = infos[1].a\n",
    "    date = infos[2].li\n",
    "    date_pub = infos[2].find('span',attrs={'itemprop':'datePublished'})\n",
    "    rating = infos[3].div.span\n",
    "    rank = infos[4]\n",
    "    description = soup1.find('div',attrs={\"itemprop\": \"description\"})\n",
    "    \n",
    "    #get categories\n",
    "    categories_table = soup1.find('div',attrs={\"class\": \"categories\"})\n",
    "    if categories_table is not None:\n",
    "        anime_info['categories'] = [i.a.contents[0] for i in categories_table.ul]\n",
    "    \n",
    "    #get related media\n",
    "    related_table = soup1.find('table',attrs={\"class\": \"pure-table pure-table-striped noHeader\"})\n",
    "    if related_table is not None:\n",
    "        anime_info['related'] = [row.td.a.contents[0] for row in related_table.find_all('tr')]\n",
    "    \n",
    "    anime_info['name'] = name\n",
    "    if altname is not None:\n",
    "        anime_info['altname'] = altname.contents[0]\n",
    "    else:\n",
    "        anime_info['altname'] = ''\n",
    "    if atype is not None:\n",
    "        anime_info['atype'] = atype.contents[0]\n",
    "    else:\n",
    "        anime_info['atype'] = ''\n",
    "    if studio is not None:\n",
    "        anime_info['studio'] = studio.contents[0]\n",
    "    else:\n",
    "        anime_info['studio'] = ''\n",
    "    if date is not None:\n",
    "        anime_info['date'] = date.a.contents[0]\n",
    "    else:\n",
    "        anime_info['date'] = ''\n",
    "    if date_pub is not None:\n",
    "        anime_info['date_pub'] = date_pub.a.contents[0]\n",
    "    else:\n",
    "        anime_info['date'] = ''\n",
    "    if rating is not None:\n",
    "        anime_info['rating'] = rating.contents[0]\n",
    "    else:\n",
    "        anime_info['rating'] = ''\n",
    "    if rank is not None:\n",
    "        anime_info['rank'] = rank.contents[0]\n",
    "    else:\n",
    "        anime_info['rank'] = ''\n",
    "    if description is not None :\n",
    "        if description.p is not None:\n",
    "            anime_info['description'] = description.p.contents[0].string\n",
    "        else:\n",
    "                anime_info['description'] = ''\n",
    "    else:\n",
    "        anime_info['description'] = ''\n",
    "    \n",
    "    if name in each_anime_info.keys():\n",
    "        name+='_2'\n",
    "        \n",
    "    each_anime_info[name] = anime_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  with open('each_anime_info.json', 'w') as fp:\n",
    "#      json.dump(each_anime_info, fp)\n",
    "\n",
    "with open('../data/each_anime_info.json','r') as data_file:    \n",
    "    each_anime_info = json.load(data_file)\n",
    "    \n",
    "anime_info = pd.DataFrame(each_anime_info).T\n",
    "anime_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#anime_info.to_csv('../data/anime_info.csv',sep='\\t',encoding='utf-8')\n",
    "anime_info = pd.read_csv('../data/anime_info.csv',sep='\\t',index_col=0)\n",
    "anime_info.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
