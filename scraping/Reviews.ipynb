{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# The \"requests\" library makes working with HTTP requests easier\n",
    "# than the built-in urllib libraries.\n",
    "import requests\n",
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = open(\"../data/anime_pages.p\",'rb') \n",
    "anime = pickle.load(test1)  \n",
    "test1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = 'http://www.anime-planet.com'\n",
    "\n",
    "all_anime = []\n",
    "for page in anime:\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    animelist = soup.find('ul',attrs={\"class\": \"cardDeck pure-g cd-narrow\"}).find_all(\"li\")\n",
    "    for ani in animelist:\n",
    "        all_anime.append(root + ani.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "CPU times: user 5min 47s, sys: 4min 43s, total: 10min 30s\n",
      "Wall time: 4h 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anime_reviews = defaultdict(list)\n",
    "stop = 0\n",
    "for i in xrange(len(all_anime)):\n",
    "    if i>=stop:\n",
    "        print i\n",
    "        stop+=500\n",
    "    review = requests.get(all_anime[i]+\"/reviews\")\n",
    "    soup1 = BeautifulSoup(review.text, \"html.parser\")\n",
    "    pages = soup1.find('section',attrs={'pure-g'})\n",
    "    if pages:\n",
    "        pages = pages.find('div',attrs='pure-1 md-4-5')\n",
    "    if pages:\n",
    "        pages = pages.find('div',attrs='pagination aligncenter')\n",
    "        if pages:\n",
    "            all_li = pages.ul.find_all('li')\n",
    "            maxpage = int(all_li[len(all_li)-2].a.contents[0])\n",
    "        else:\n",
    "            maxpage=1\n",
    "    else:\n",
    "        maxpage=1\n",
    "    for j in xrange(1,maxpage+1):\n",
    "        anime_reviews[all_anime[i].split('/')[-1]].append(requests.get(all_anime[i]+\"/reviews?page=\"+str(j)))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle.dump(anime_reviews, open(\"../data/anime_reviews1.p\", \"wb\"))\n",
    "\n",
    "test3 = open(\"../data/anime_reviews1.p\",'rb') \n",
    "anime_reviews = pickle.load(test3)  \n",
    "test3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewsdict = {}\n",
    "for anime in anime_reviews.keys():\n",
    "    animedict = {anime:{}}\n",
    "    for a in range(len(anime_reviews[anime])):\n",
    "        soup = BeautifulSoup(anime_reviews[anime][a].text, \"html.parser\")\n",
    "        body = soup.find('div',attrs={\"class\": \"pure-1 md-4-5\"})\n",
    "        if body is not None:\n",
    "            sections = body.find_all('section', attrs={'class':'pure-g'})\n",
    "            names = [i.find('a',attrs={'itemprop':'author'}).contents[0] for i in sections if i.find('a',attrs={'itemprop':'author'}) is not None]\n",
    "            scores = [i.find_all('div',attrs={'class':'pure-1-5'}) for i in sections if i.find_all('div',attrs={'class':'pure-1-5'}) != []]\n",
    "            story = [i[0].contents[0] for i in scores]\n",
    "            animation = [i[1].contents[0] for i in scores]\n",
    "            sound = [i[2].contents[0] for i in scores]\n",
    "            characters = [i[3].contents[0] for i in scores]\n",
    "            overall = [i[4].contents[0] for i in scores]\n",
    "            reviews = [i.contents[1].string for i in body.find_all('section', attrs={'class':'userContent readMore'})]\n",
    "            anime_name = soup.find('h1',attrs={\"itemprop\": \"name\"}).contents*len(names)\n",
    "            temp = dict(zip(names,zip(story,animation,sound,characters,overall,reviews,anime_name)))\n",
    "            animedict[anime].update(temp)\n",
    "    reviewsdict.update(animedict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/all_reviews.json', 'w') as fp:\n",
    "      json.dump(reviewsdict, fp)\n",
    "\n",
    "with open('../data/all_reviews.json','r') as data_file:    \n",
    "    reviewsdict = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewlist = []\n",
    "for i in reviewsdict.keys():\n",
    "    reviewlist+=reviewsdict[i].keys()\n",
    "users = list(set(reviewlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4462, 17194)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users),len(reviewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newlist = []\n",
    "\n",
    "for key in reviewsdict:\n",
    "    for user in reviewsdict[key]:\n",
    "        story = reviewsdict[key][user][0]\n",
    "        animation = reviewsdict[key][user][1]\n",
    "        sound = reviewsdict[key][user][2]\n",
    "        character = reviewsdict[key][user][3]\n",
    "        overall = reviewsdict[key][user][4]\n",
    "        reviews = reviewsdict[key][user][5]\n",
    "        anime_name = reviewsdict[key][user][6]\n",
    "        temp = {\"name\":user,'anime':key,'story':story,'animation':animation,'sound':sound,'character':character,'overall':overall,'reviews':reviews,'anime_name':anime_name}\n",
    "        newlist.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17194, 9)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = pd.DataFrame(newlist)\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata.to_csv('../data/all_reviews.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#newdata.loc[:,['anime','anime_name']].to_csv('../data/anime_name.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 26.8 s, total: 3min 44s\n",
      "Wall time: 1h 46min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = 'http://www.anime-planet.com/users/'\n",
    "user_profiles = {}\n",
    "for i in xrange(len(users)):\n",
    "    user = requests.get(root+users[i])\n",
    "    soup = BeautifulSoup(user.text, \"html.parser\")\n",
    "    user_profiles[users[i]]=soup\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "each_user_info = {}\n",
    "\n",
    "for user in user_profiles.keys():\n",
    "    test = user_profiles[user]\n",
    "    user_info = {}\n",
    "\n",
    "    info = test.find('div',attrs={'class':'pure-u-3-5 pure-u-md-4-5'})\n",
    "    name = info.find('h2',attrs={'id':'profileName'}).find_all('a')[1].contents[0]\n",
    "\n",
    "    stats = info.find('ul',attrs={'class':'userStats'}).findAll('li')\n",
    "    place = \"\"\n",
    "    age_sex = \"\"\n",
    "    join = \"\"\n",
    "\n",
    "    # find place, age, sex if exist\n",
    "    for li in stats:\n",
    "        if li.findAll(attrs={\"class\" : \"fa fa-home\"}):\n",
    "            place = li.contents[1]\n",
    "        elif li.findAll(attrs={\"class\" : \"fa fa-calendar\"}):\n",
    "            join = li.contents[1]\n",
    "        elif li.findAll(attrs={\"class\" : \"fa fa-user\"}):\n",
    "            age_sex = li.contents[2].replace('\\n','')\n",
    "\n",
    "    each_user_info[name] = {\"place\":place,\"age_sex\":age_sex,\"join\":join}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('../data/all_users.json', 'w') as fp:\n",
    "#        json.dump(each_user_info, fp)\n",
    "\n",
    "with open('../data/all_users.json','r') as data_file:    \n",
    "    each_user_info = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_sex</th>\n",
       "      <th>join</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00081</th>\n",
       "      <td>26</td>\n",
       "      <td>Joined Jan 16, 2012</td>\n",
       "      <td>Death Star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0Marcandre</th>\n",
       "      <td>20 / M</td>\n",
       "      <td>Joined Mar 31, 2013</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0oDaano0</th>\n",
       "      <td>?</td>\n",
       "      <td>Joined Apr 9, 2013</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0utl4w</th>\n",
       "      <td>?</td>\n",
       "      <td>Joined Mar 31, 2010</td>\n",
       "      <td>somewhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11bowman</th>\n",
       "      <td>21 / M</td>\n",
       "      <td>Joined Jun 19, 2010</td>\n",
       "      <td>Amsterdam, Netherlands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age_sex                 join                   place\n",
       "00081           26   Joined Jan 16, 2012              Death Star\n",
       "0Marcandre  20 / M   Joined Mar 31, 2013                        \n",
       "0oDaano0         ?    Joined Apr 9, 2013                        \n",
       "0utl4w           ?   Joined Mar 31, 2010               somewhere\n",
       "11bowman    21 / M   Joined Jun 19, 2010  Amsterdam, Netherlands"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf = pd.DataFrame(each_user_info).T\n",
    "userdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userdf.to_csv('../data/all_users.csv',sep='\\t',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
